# retail-analytics-data-engineering-
End to End Azure Data Engineering project using ADF, Databricks, ADLS, and Power BI
End-to-End Azure Data Engineering Project

ğŸ“Œ Project Overview

This project demonstrates a production-style retail analytics data platform built using Azure Data Factory, Azure Data Lake Storage (ADLS), Databricks, and Power BI following the Medallion Architecture (Bronze â†’ Silver â†’ Gold).

The goal was to ingest raw retail transaction data, clean and transform it into analytics-ready datasets, and finally visualize business KPIs through an interactive Power BI dashboard.


ğŸ¯ Problem Statement

Retail organizations generate large volumes of transactional data that is often:
	â€¢	Fragmented
	â€¢	Unclean
	â€¢	Difficult to analyze in real time

The challenge was to:
	â€¢	Build a scalable ingestion pipeline
	â€¢	Ensure data quality & reliability
	â€¢	Enable business-friendly dashboards for decision-making


ğŸ—ï¸ Solution Architecture

The solution follows a layered data platform design:

ğŸ”¹ Architecture Flow

Source â†’ ADLS (Bronze) â†’ ADF â†’ Databricks (Silver & Gold) â†’ Power BI

ğŸ”¹ Medallion Architecture
	â€¢	Bronze (Raw Layer)
Raw, immutable data ingestion using Azure Data Factory into ADLS.
	â€¢	Silver (Cleaned Layer)
Data cleansing, deduplication, null handling, return logic, and standardization using Databricks (PySpark).
	â€¢	Gold (Business Layer)
Aggregated KPIs and analytics-ready tables optimized for reporting.
	â€¢	Consumption Layer
Power BI dashboards connected directly to Gold tables.

ğŸ“Œ (Architecture diagram included in this repository)


ğŸ§° Tech Stack
	â€¢	Azure Data Factory (ADF) â€“ Ingestion & orchestration
	â€¢	Azure Data Lake Storage Gen2 (ADLS) â€“ Data storage
	â€¢	Azure Databricks (PySpark + Delta Lake) â€“ Processing & transformations
	â€¢	Power BI Desktop â€“ Data visualization & dashboards
	â€¢	Delta Lake â€“ ACID-compliant storage for analytics


ğŸ”„ Data Pipeline Breakdown

1ï¸âƒ£ Bronze Layer â€“ Raw Ingestion
	â€¢	Raw retail CSV data ingested via ADF Copy Activity
	â€¢	Stored in ADLS Bronze container
	â€¢	Schema-on-read approach
	â€¢	Data remains unchanged & immutable

ğŸ“‚ Notebook: Bronze_Ingestion.ipynb

2ï¸âƒ£ Silver Layer â€“ Data Cleaning & Transformation

Performed using Databricks (PySpark):
	â€¢	Removed duplicates
	â€¢	Handled null customer IDs
	â€¢	Identified returns using negative quantities
	â€¢	Standardized date & time columns
	â€¢	Created derived fields:
	â€¢	is_return
	â€¢	line_total
	â€¢	Time-based attributes (Year, Month, Hour, DayOfWeek)

ğŸ“‚ Notebook: Silver_Transformation.ipynb

3ï¸âƒ£ Gold Layer â€“ Business Aggregations

Created analytics-ready tables such as:
	â€¢	Revenue by product
	â€¢	Revenue by country
	â€¢	Customer-level KPIs
	â€¢	Monthly & yearly revenue trends
	â€¢	Return rate analysis
	â€¢	Time-based sales density
	â€¢	Pareto (80/20) product analysis

All outputs stored as Delta tables for BI consumption.

ğŸ“‚ Notebook: Gold.ipynb

ğŸ“Š Power BI Dashboard

An interactive Retail Analytics Dashboard built using Gold tables.


Key KPIs Visualized:
	â€¢	ğŸ’° Total Revenue
	â€¢	ğŸ§¾ Total Orders
	â€¢	ğŸ“¦ Units Sold
	â€¢	â° Sales Density by Hour
	â€¢	ğŸ“ˆ Monthly Revenue Trends
	â€¢	ğŸŒ Sales by Country
	â€¢	ğŸ” Return Rate by Product
	â€¢	ğŸ“† Sales by Day / Month / Year

ğŸ“¸ (Dashboard screenshot included in this repository)


ğŸš§ Challenges Faced
	â€¢	Handling negative quantities for return transactions
	â€¢	Managing null customer identifiers
	â€¢	Designing reusable transformations
	â€¢	Ensuring schema consistency across layers
	â€¢	Optimizing data for BI performance


ğŸ“Œ Key Learnings
	â€¢	Designing scalable ETL pipelines
	â€¢	Implementing Medallion Architecture
	â€¢	Delta Lake best practices
	â€¢	Data modeling for BI
	â€¢	End-to-end cloud data engineering workflow


ğŸš€ Future Enhancements
	â€¢	Incremental data loading
	â€¢	Streaming ingestion
	â€¢	Row-level security
	â€¢	Power BI Service deployment
	â€¢	Automated refresh scheduling
